Dataset used here can be downloaded from "https://drive.google.com/file/d/1lZNjlivBPOWe9ln2Pik5bBsTj-Vrf2e6/view?usp=sharing" or you can directly downloaded from kaggle "https://www.kaggle.com/fireballbyedimyrnmom/us-counties-covid-19-dataset".

# Pyspark-
Apache Spark is written in Scala programming language. PySpark has been released in order to support the collaboration of Apache Spark and Python, it actually is a Python API for Spark. In addition, PySpark, helps you interface with Resilient Distributed Datasets (RDDs) in Apache Spark and Python programming language. This has been achieved by taking advantage of the Py4j library. PySpark LogoPy4J is a popular library which is integrated within PySpark and allows python to dynamically interface with JVM objects. PySpark features quite a few libraries for writing efficient programs.
# Following topics have been covered here:
* Setting up the environment
* Loading the dataset into Pyspark
* Applying Filters
* Group by and Aggregation
* Joins (left, right, inner, full)
* Partition By & Window Function
* Automating the code
# Different Types of SQL JOINs
Here are the different types of the JOINs in SQL:
![sql-joins](https://user-images.githubusercontent.com/41945372/113957252-7f87d480-983c-11eb-88c7-5a8ce259ac96.png)
* (INNER) JOIN: Returns records that have matching values in both tables
* LEFT (OUTER) JOIN: Returns all records from the left table, and the matched records from the right table
* RIGHT (OUTER) JOIN: Returns all records from the right table, and the matched records from the left table
* FULL (OUTER) JOIN: Returns all records when there is a match in either left or right table
